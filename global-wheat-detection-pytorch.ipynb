{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sonujha090/global-wheat-detection-pytorch?scriptVersionId=115151610\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Aim: To Create A Pipeline For Object Detection With Pytorch","metadata":{}},{"cell_type":"markdown","source":"## Importing the necessary Libraries","metadata":{}},{"cell_type":"code","source":"# Download TorchVision repo to use some files from\n# references/detection\n!pip install pycocotools --quiet\n!git clone https://github.com/pytorch/vision.git\n!git checkout v0.3.0\n\n!cp vision/references/detection/utils.py ./\n!cp vision/references/detection/transforms.py ./\n!cp vision/references/detection/coco_eval.py ./\n!cp vision/references/detection/engine.py ./\n!cp vision/references/detection/coco_utils.py ./","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:05:12.669265Z","iopub.execute_input":"2022-12-31T08:05:12.669611Z","iopub.status.idle":"2022-12-31T08:06:53.778315Z","shell.execute_reply.started":"2022-12-31T08:05:12.669579Z","shell.execute_reply":"2022-12-31T08:06:53.776859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch, torchvision\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:08:18.249452Z","iopub.execute_input":"2022-12-31T08:08:18.250244Z","iopub.status.idle":"2022-12-31T08:08:21.862611Z","shell.execute_reply.started":"2022-12-31T08:08:18.250194Z","shell.execute_reply":"2022-12-31T08:08:21.861611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport ast \nfrom PIL import Image\nimport os ","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:08:21.864911Z","iopub.execute_input":"2022-12-31T08:08:21.866Z","iopub.status.idle":"2022-12-31T08:08:21.871562Z","shell.execute_reply.started":"2022-12-31T08:08:21.865958Z","shell.execute_reply":"2022-12-31T08:08:21.870199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"path = Path('/kaggle/input/global-wheat-detection')","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:08:21.873252Z","iopub.execute_input":"2022-12-31T08:08:21.873652Z","iopub.status.idle":"2022-12-31T08:08:21.886171Z","shell.execute_reply.started":"2022-12-31T08:08:21.873613Z","shell.execute_reply":"2022-12-31T08:08:21.885178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(path/'train.csv')\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:08:21.889855Z","iopub.execute_input":"2022-12-31T08:08:21.890514Z","iopub.status.idle":"2022-12-31T08:08:22.175644Z","shell.execute_reply.started":"2022-12-31T08:08:21.890478Z","shell.execute_reply":"2022-12-31T08:08:22.174739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:08:22.177084Z","iopub.execute_input":"2022-12-31T08:08:22.177406Z","iopub.status.idle":"2022-12-31T08:08:22.199092Z","shell.execute_reply.started":"2022-12-31T08:08:22.177379Z","shell.execute_reply":"2022-12-31T08:08:22.198133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cvt2list = lambda x: ast.literal_eval(x) \ndf['bbox'] = df['bbox'].apply(cvt2list)","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:08:22.201639Z","iopub.execute_input":"2022-12-31T08:08:22.202229Z","iopub.status.idle":"2022-12-31T08:08:24.061881Z","shell.execute_reply.started":"2022-12-31T08:08:22.202193Z","shell.execute_reply":"2022-12-31T08:08:24.060853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conv = lambda x: [x[0], x[1], x[0]+x[2], x[1]+x[3]]\ndf['bbox'] = df['bbox'].apply(conv)","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:08:24.063321Z","iopub.execute_input":"2022-12-31T08:08:24.063763Z","iopub.status.idle":"2022-12-31T08:08:24.29079Z","shell.execute_reply.started":"2022-12-31T08:08:24.063704Z","shell.execute_reply":"2022-12-31T08:08:24.289707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:08:24.293292Z","iopub.execute_input":"2022-12-31T08:08:24.293614Z","iopub.status.idle":"2022-12-31T08:08:24.307359Z","shell.execute_reply.started":"2022-12-31T08:08:24.293584Z","shell.execute_reply":"2022-12-31T08:08:24.306288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Unique Images**","metadata":{}},{"cell_type":"code","source":"df_processed= df.groupby('image_id')['bbox'].apply(list).reset_index(name='bboxes')","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:08:24.442054Z","iopub.execute_input":"2022-12-31T08:08:24.442639Z","iopub.status.idle":"2022-12-31T08:08:24.546753Z","shell.execute_reply.started":"2022-12-31T08:08:24.442599Z","shell.execute_reply":"2022-12-31T08:08:24.545711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_processed.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:08:25.307686Z","iopub.execute_input":"2022-12-31T08:08:25.308385Z","iopub.status.idle":"2022-12-31T08:08:25.366291Z","shell.execute_reply.started":"2022-12-31T08:08:25.308332Z","shell.execute_reply":"2022-12-31T08:08:25.365146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Splitting the dataset into train and valid set**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split the data into independent and dependent variables\nX = df_processed.drop(columns=['bboxes'])\ny = df_processed['bboxes']\n\n# Split the data into train and validation sets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create the train and validation dataframes\ntrain_df = pd.concat([X_train, y_train], axis=1)\nvalid_df = pd.concat([X_valid, y_valid], axis=1)\ntrain_df.shape, valid_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:08:26.255763Z","iopub.execute_input":"2022-12-31T08:08:26.256173Z","iopub.status.idle":"2022-12-31T08:08:26.27479Z","shell.execute_reply.started":"2022-12-31T08:08:26.256138Z","shell.execute_reply":"2022-12-31T08:08:26.273406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # taking a small sample for experiment\n# train_df = train_df.sample(frac=0.1)\n# valid_df = valid_df.sample(frac=0.1)\n# train_df.shape, valid_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:08:39.815491Z","iopub.execute_input":"2022-12-31T08:08:39.815896Z","iopub.status.idle":"2022-12-31T08:08:39.826007Z","shell.execute_reply.started":"2022-12-31T08:08:39.815863Z","shell.execute_reply":"2022-12-31T08:08:39.824811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pytorch DataLoader","metadata":{}},{"cell_type":"code","source":"class WheatDataset(Dataset):\n    def __init__(self, df, root, transform=None):\n        self.df = df\n        self.root = Path(root)\n        self.transforms = transform\n        self.image_ids = self.df.image_id.unique()\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        img_id, bboxes = self.df.iloc[idx]\n        img_path = os.path.join(self.root, img_id+'.jpg')\n        img = Image.open(img_path)\n        boxes = []\n        areas = []\n        for bbox in bboxes:\n            x0 = bbox[0]\n            y0 = bbox[1]\n            x1 = bbox[2]\n            y1 = bbox[3]\n            boxes.append([x0, y0, x1, y1])\n            areas.append((x1-x0)*(y1-y0))\n            \n        boxes = np.array(boxes)\n        boxes = torch.tensor(bboxes, dtype=torch.float32)\n        areas= torch.tensor(areas)\n        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n        target = {'boxes': boxes, 'labels':torch.ones(len(boxes), dtype=torch.int64), 'image_id':torch.tensor(idx), 'area': areas, 'iscrowd': iscrowd}\n\n        if self.transforms is not None:\n            img = self.transforms(img)\n            \n        return img, target","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:08:40.610735Z","iopub.execute_input":"2022-12-31T08:08:40.611455Z","iopub.status.idle":"2022-12-31T08:08:40.621644Z","shell.execute_reply.started":"2022-12-31T08:08:40.611414Z","shell.execute_reply":"2022-12-31T08:08:40.620579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds = WheatDataset(train_df, path/'train')","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:08:41.008159Z","iopub.execute_input":"2022-12-31T08:08:41.008961Z","iopub.status.idle":"2022-12-31T08:08:41.015189Z","shell.execute_reply.started":"2022-12-31T08:08:41.008918Z","shell.execute_reply":"2022-12-31T08:08:41.01409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize a dataset","metadata":{}},{"cell_type":"code","source":"def plot_bboxes(img, target):\n    # Get the image and bounding box information\n    image = img\n    bboxes = target['boxes']\n    category_idx = target['labels']\n    category_names = ['0', '1']\n\n    # Create a figure and axis\n    fig, ax = plt.subplots(figsize=(10,10))\n    \n\n    # Display the image\n    ax.imshow(image)\n\n    # Add a bounding box for each object in the image\n    for bbox, idx in zip(bboxes, category_idx):\n        x, y, width, height = bbox\n        rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor='r', facecolor='none')\n        ax.add_patch(rect)\n#         ax.text(x, y, category_names[idx], color='w', fontsize=12, bbox=dict(facecolor='r', alpha=0.5))\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:08:41.804978Z","iopub.execute_input":"2022-12-31T08:08:41.805373Z","iopub.status.idle":"2022-12-31T08:08:41.812523Z","shell.execute_reply.started":"2022-12-31T08:08:41.80534Z","shell.execute_reply":"2022-12-31T08:08:41.811204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img, target = ds[1]\nplot_bboxes(img, target)","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:08:42.16052Z","iopub.execute_input":"2022-12-31T08:08:42.161222Z","iopub.status.idle":"2022-12-31T08:08:42.805214Z","shell.execute_reply.started":"2022-12-31T08:08:42.161183Z","shell.execute_reply":"2022-12-31T08:08:42.803755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Transform**","metadata":{}},{"cell_type":"code","source":"import torchvision.transforms as T\n\ndef get_transform(train):\n    transforms = []\n    transforms.append(T.PILToTensor())\n    transforms.append(T.ConvertImageDtype(torch.float))\n    if train:\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    return T.Compose(transforms)","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:08:42.934404Z","iopub.execute_input":"2022-12-31T08:08:42.935123Z","iopub.status.idle":"2022-12-31T08:08:42.942326Z","shell.execute_reply.started":"2022-12-31T08:08:42.935075Z","shell.execute_reply":"2022-12-31T08:08:42.941336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = WheatDataset(train_df, path/'train', transform=get_transform(train=True))\nvalid_ds = WheatDataset(valid_df, path/'train', transform=get_transform(train=False))","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:08:43.449314Z","iopub.execute_input":"2022-12-31T08:08:43.450039Z","iopub.status.idle":"2022-12-31T08:08:43.457009Z","shell.execute_reply.started":"2022-12-31T08:08:43.450001Z","shell.execute_reply":"2022-12-31T08:08:43.455751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**DataLoader**","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_dl = DataLoader(train_ds, collate_fn=collate_fn, batch_size=8)\nvalid_dl = DataLoader(valid_ds, collate_fn=collate_fn, batch_size=8)","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:08:44.311414Z","iopub.execute_input":"2022-12-31T08:08:44.312284Z","iopub.status.idle":"2022-12-31T08:08:44.318605Z","shell.execute_reply.started":"2022-12-31T08:08:44.312234Z","shell.execute_reply":"2022-12-31T08:08:44.317269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:08:45.236826Z","iopub.execute_input":"2022-12-31T08:08:45.237494Z","iopub.status.idle":"2022-12-31T08:08:45.361497Z","shell.execute_reply.started":"2022-12-31T08:08:45.237458Z","shell.execute_reply":"2022-12-31T08:08:45.36051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n\nnum_classes = 2  # 1 class (wheat) + background\n\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:08:45.646952Z","iopub.execute_input":"2022-12-31T08:08:45.647289Z","iopub.status.idle":"2022-12-31T08:08:52.963549Z","shell.execute_reply.started":"2022-12-31T08:08:45.647259Z","shell.execute_reply":"2022-12-31T08:08:52.962509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Testing on one batch**","metadata":{}},{"cell_type":"code","source":"images, targets = next(iter(train_dl))\n\nfor images, targets in train_dl:\n\n    images = list(image.to(device) for image in images)\n    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n    loss_dict = model(images, targets)\n    print(loss_dict)\n    break","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:08:52.965647Z","iopub.execute_input":"2022-12-31T08:08:52.96663Z","iopub.status.idle":"2022-12-31T08:09:05.337362Z","shell.execute_reply.started":"2022-12-31T08:08:52.966578Z","shell.execute_reply":"2022-12-31T08:09:05.336103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Hyperparameter**","metadata":{"execution":{"iopub.status.busy":"2022-12-30T13:16:38.354343Z","iopub.execute_input":"2022-12-30T13:16:38.357295Z","iopub.status.idle":"2022-12-30T13:16:46.979418Z","shell.execute_reply.started":"2022-12-30T13:16:38.357257Z","shell.execute_reply":"2022-12-30T13:16:46.978399Z"}}},{"cell_type":"code","source":"# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                            momentum=0.9, weight_decay=0.0005)\n\n# and a learning rate scheduler which decreases the learning rate by\n# 10x every 3 epochs\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:09:05.339112Z","iopub.execute_input":"2022-12-31T08:09:05.339477Z","iopub.status.idle":"2022-12-31T08:09:05.346579Z","shell.execute_reply.started":"2022-12-31T08:09:05.339439Z","shell.execute_reply":"2022-12-31T08:09:05.345505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"import math\nimport sys\nimport time\n\nimport torch\nimport torchvision.models.detection.mask_rcnn\nimport utils\nfrom coco_eval import CocoEvaluator\nfrom coco_utils import get_coco_api_from_dataset\n\n\ndef train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler=None):\n    model.train()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n    header = f\"Epoch: [{epoch}]\"\n\n    lr_scheduler = None\n    if epoch == 0:\n        warmup_factor = 1.0 / 1000\n        warmup_iters = min(1000, len(data_loader) - 1)\n\n#         lr_scheduler = torch.optim.lr_scheduler.linear_lr(\n#             optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n#         )\n    lr_scheduler = torch.optim.lr_scheduler.StepLR(\n        optimizer, step_size=10\n    )\n\n    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        with torch.cuda.amp.autocast(enabled=scaler is not None):\n            loss_dict = model(images, targets)\n            losses = sum(loss for loss in loss_dict.values())\n\n        # reduce losses over all GPUs for logging purposes\n        loss_dict_reduced = utils.reduce_dict(loss_dict)\n        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n\n        loss_value = losses_reduced.item()\n\n        if not math.isfinite(loss_value):\n            print(f\"Loss is {loss_value}, stopping training\")\n            print(loss_dict_reduced)\n            sys.exit(1)\n\n        optimizer.zero_grad()\n        if scaler is not None:\n            scaler.scale(losses).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            losses.backward()\n            optimizer.step()\n\n        if lr_scheduler is not None:\n            lr_scheduler.step()\n\n        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n\n    return ","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:09:05.349448Z","iopub.execute_input":"2022-12-31T08:09:05.350124Z","iopub.status.idle":"2022-12-31T08:09:05.914648Z","shell.execute_reply.started":"2022-12-31T08:09:05.35008Z","shell.execute_reply":"2022-12-31T08:09:05.913618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\n# training for 5 epochs\nfrom engine import evaluate\nnum_epochs = 5\n\nfor epoch in range(num_epochs):\n    # training for one epoch\n    train_one_epoch(model, optimizer, train_dl, device, epoch, print_freq=10)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    evaluate(model, valid_dl, device=device)","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:09:05.916223Z","iopub.execute_input":"2022-12-31T08:09:05.916595Z","iopub.status.idle":"2022-12-31T08:10:29.219591Z","shell.execute_reply.started":"2022-12-31T08:09:05.916558Z","shell.execute_reply":"2022-12-31T08:10:29.218278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving the model\ntorch.save(model.state_dict(), 'fasterrcnn_resnet50_fpn.pth')","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:11:09.892627Z","iopub.execute_input":"2022-12-31T08:11:09.893373Z","iopub.status.idle":"2022-12-31T08:11:10.24288Z","shell.execute_reply.started":"2022-12-31T08:11:09.893335Z","shell.execute_reply":"2022-12-31T08:11:10.241799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the function takes the original prediction and the iou threshold.\n\ndef apply_nms(orig_prediction, iou_thresh=0.3):\n    \n    # torchvision returns the indices of the bboxes to keep\n    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n    \n    final_prediction = orig_prediction\n    final_prediction['boxes'] = final_prediction['boxes'][keep]\n    final_prediction['scores'] = final_prediction['scores'][keep]\n    final_prediction['labels'] = final_prediction['labels'][keep]\n    \n    return final_prediction\n\n# function to convert a torchtensor back to PIL image\ndef torch_to_pil(img):\n    return T.ToPILImage()(img).convert('RGB')\n","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:11:11.654293Z","iopub.execute_input":"2022-12-31T08:11:11.655528Z","iopub.status.idle":"2022-12-31T08:11:11.663085Z","shell.execute_reply.started":"2022-12-31T08:11:11.655474Z","shell.execute_reply":"2022-12-31T08:11:11.661902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing on one image","metadata":{}},{"cell_type":"code","source":"# pick one image from the test set\nimg, target = valid_ds[5]\n# put the model in evaluation mode\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])[0]\n    \nprint('predicted #boxes: ', len(prediction['labels']))","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:11:14.310844Z","iopub.execute_input":"2022-12-31T08:11:14.311946Z","iopub.status.idle":"2022-12-31T08:11:14.473374Z","shell.execute_reply.started":"2022-12-31T08:11:14.311907Z","shell.execute_reply":"2022-12-31T08:11:14.472198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_bboxes(img, target):\n    # Get the image and bounding box information\n    image = img\n    bboxes = target['boxes'].cpu()\n    category_idx = target['labels'].cpu()\n    category_names = ['0', '1']\n\n    # Create a figure and axis\n    fig, ax = plt.subplots(figsize=(10,10))\n    \n\n    # Display the image\n    ax.imshow(image)\n\n    # Add a bounding box for each object in the image\n    for bbox, idx in zip(bboxes, category_idx):\n        x, y, width, height = bbox\n        rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor='r', facecolor='none')\n        ax.add_patch(rect)\n#         ax.text(x, y, category_names[idx], color='w', fontsize=12, bbox=dict(facecolor='r', alpha=0.5))\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:11:18.040004Z","iopub.execute_input":"2022-12-31T08:11:18.040368Z","iopub.status.idle":"2022-12-31T08:11:18.047997Z","shell.execute_reply.started":"2022-12-31T08:11:18.040337Z","shell.execute_reply":"2022-12-31T08:11:18.046719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('MODEL OUTPUT')\n\nplot_bboxes(torch_to_pil(img), prediction)","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:11:20.059202Z","iopub.execute_input":"2022-12-31T08:11:20.059586Z","iopub.status.idle":"2022-12-31T08:11:20.754432Z","shell.execute_reply.started":"2022-12-31T08:11:20.059553Z","shell.execute_reply":"2022-12-31T08:11:20.749968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nms_prediction = apply_nms(prediction, iou_thresh=0.2)\nprint('NMS APPLIED MODEL OUTPUT')\nplot_bboxes(torch_to_pil(img), nms_prediction)","metadata":{"execution":{"iopub.status.busy":"2022-12-31T08:11:23.395581Z","iopub.execute_input":"2022-12-31T08:11:23.396361Z","iopub.status.idle":"2022-12-31T08:11:24.006611Z","shell.execute_reply.started":"2022-12-31T08:11:23.396321Z","shell.execute_reply":"2022-12-31T08:11:24.005682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference Kernal","metadata":{}},{"cell_type":"markdown","source":"**https://www.kaggle.com/code/sonujha090/submission-fasterrcnn/**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}